<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
	<style>
		body{
			color: #333;
		}
		#container{
			min-width: 1000px;
			width: 1000px;
/*			overflow: auto;
*/			margin: 50px auto;padding: 30px;
			/*zoom: 1;*/
*//*			border: 1px solid #ccc;background: #fc9;color: #fff;
*/		}
		#left{
			float: left;
			width: 230px;
			height: 250px;
			margin-left: 0px;
		}
		#right{
			float: left;
			width: auto;
			margin-left: 30px;
		}
		#name{
			font-size: 22.0pt;
		    mso-bidi-font-size: 24.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		        font-weight: bold;
		}
		#info{
		    font-size: 16.0pt;
		    mso-bidi-font-size: 16.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    margin-top: 30px;
		    margin-left: 5px;
		    margin-bottom: 10px;
		    
		}
		.clear{clear:both; height: 0; line-height: 0; font-size: 0}
		.Bio{
			font-size:16.0pt;
			mso-bidi-font-size:17.0pt;
			line-height:150%;
			font-family:Times;
			mso-bidi-font-family:Lato-Regular;
			text-align: justify;
		}
		span.SpellE {
		    mso-style-name: "";
		    mso-spl-e: yes;
		}
		span.Title{
			    font-size: 22.0pt;
			    mso-bidi-font-size: 17.0pt;
			    font-family: Times;
			    mso-bidi-font-family: Lato-Regular;
			    font: bold;
			    margin-top: 10px;
			    width: 1000px;
		}
		div.section{
			padding-top: 30px;
		}
		
		div.sub-left{
			float: left;
			width: 250px;
						
		}
		div.sub-left img{
			vertical-align: middle;
			horizontal-align: middle;
			margin-top: 10px;
		}
		
		div.sub-left span{
			height: 100%;
			display: inline-block;
			vertical-align: top;
						
		}
		div.sub-right{
			float: left;
			width: 750px;			
		}
		.paper{
			overflow: auto;
			zoom:1;
			border-top: 2px ;
			padding-bottom: 0px;
			min-height: 160px;

		}
		.paperTitle{
			font-size:14pt;
			mso-bidi-font-size:14pt;
			font-family:Calibri;
			mso-bidi-font-family:Calibri;
			margin-top: 10px;
			margin-bottom: 5px;
			font-weight: bold;
		}
		.paperName{
		    font-size: 12pt;
		    mso-bidi-font-size: 12pt;		    
		    font-family: Calibri;
		    mso-bidi-font-family: Times;
		    line-height:160%;
		    font-style: italic;
		}		
		.paperPub{
		    font-size: 14pt;
		    mso-bidi-font-size: 14pt;		    
		    font-family: Calibri;
		    mso-bidi-font-family: Times;		    
		    font-style: italic;
		    line-height:160%;
		}
		.paperLink{
		    font-size: 13.0pt;
		    mso-bidi-font-size: 13.0pt;		    
		    font-family: Calibri;
		    mso-bidi-font-family: Times;
		    line-height:170%;
		}
		.special{
		    margin-top: 0in;
		    margin-bottom: 0in;
		    margin-left: -.9pt;
		    margin-bottom: .0001pt;
		    text-indent: .9pt;
		    mso-pagination: none;
		    tab-stops: 13.75in;
		    mso-layout-grid-align: none;
		    text-autospace: none;
		}
		.long div.sub-left, .long div.sub-right{
			height: 300px;
			width: 980px;

		}
		.short div.sub-left, .short div.sub-right{
			height:150px;

		}
		div.sub-left,div.sub-right{
			height:200px;

		}
	</style>
</head>
<body>
	<div id="container">
		<div id="left">			
			<img width="200" height="230" src="imgs/dkliang.jpg">
		</div>
		<div id="right">
			<div id="name"><b>Dingkang Liang</b> (梁定康) </div>
			<div id="info">
				PHD student<p>
				Huazhong University of Science and Technology<p>
				Email: dkliang@hust.edu.cn<p>				
			</div>
		
			         <a href="https://github.com/dk-liang" target="_blank" rel="nofollow"><span>Github</span></a>  |
				 <a href="https://scholar.google.com/citations?user=Tre69v0AAAAJ&hl=zh-CN" target="_blank" rel="nofollow"><span>Google Scholar</span></a>  
			
			</div>

		<div class="clear"></div>
		<div class="section">
			<span class="Title"><b>Brief Bio</b></span><p>			
				<div class="Bio">
					I am currently a PHD student at Huazhong University of Science and Technology, under the supervision of Prof. <a href="https://scholar.google.com/citations?user=UeltiQ4AAAAJ&hl=zh-CN" target="_blank" rel="nofollow">Xiang Bai </a>.
					My research interests mainly lie in 3D object detection, crowd analysis.
				</div>



	<div class="section">
		<span class="Title"><b>News</b></span>
		<p>
		<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		<div class="paper long"><b>
			<div class="sub-right">
			<div class="paperName"><b>    
				04 / 2024: I obtain the Youth Student Fundamental Research Project from the National Natural Science Foundation of China (<span style="color:red"> <b>首批国家自然科学基金博士生项目</b></span>), which is a grant of <b>300,000 RMB</b>.<br>
				03 / 2024: FIDTM is selected as an <span style="color:red">ESI Highly Cited Paper</span> (Top 1% of papers in the academic field)<br>
				03 / 2024: One paper is accepted by <span style="color:red">PR</span><br>
				02 / 2024: One paper is accepted by <span style="color:red">CVPR 2024</span><br>
				09 / 2023: <b>Outstanding reviewer at ICCV 2023</b>
				09 / 2023: One paper is accepted by <span style="color:red">NeurIPS 2023</span><br>
				09 / 2023: TransCrowd is selected as an <span style="color:red">ESI Highly Cited Paper</span> (Top 1% of papers in the academic field)<br>
				09 / 2023: One paper is accepted by <span style="color:red">IEEE TII</span><br>
				08 / 2023: One paper is accepted by <span style="color:red">IEEE RAL</span><br>
				08 / 2023: One paper is accepted by <span style="color:red">PRCV 2023</span><br>
				07 / 2023: One paper is accepted by <span style="color:red">ICCV 2023</span><br>
				04 / 2023: One paper is accepted by <span style="color:red">ICDAR 2023</span><br>
				03 / 2023: <b>Two</b> papers are accepted by <span style="color:red">CVPR 2023</span><br>
				02 / 2023: One paper is accepted by <span style="color:red">ICASSP 2023</span><br>
				01 / 2023: One paper is accepted by <span style="color:red">ICRA 2023</span><br>
				09 / 2022: Guided graduate students win <b>1st</b> place in the VisDrone2022 (PRCV) challenge on the crowd counting track.<br>
				08 / 2022: One paper is accepted by <span style="color:red">IEEE TMM</span><br>
				06 / 2022: Two papers are accepted by <span style="color:red">ECCV 2022</span><br>
				03 / 2022: We released the first comprehensive public African text dataset [<a href="https://dk-liang.github.io/HUST-ASTD/" target="_blank" rel="nofollow">project</a>]<br>
				10 / 2021: I'm awarded <b>National Scholarship</b><br>
				10 / 2021: One paper is accepted by <span style="color:red">IJCV</span><br>
				09 / 2021: We win <b>1st</b> place in the VisDrone2021 (ICCV) challenge on the crowd counting track.<br>
			</b></div>
			</b></div>
		</b></div>
	</div>
				

	
	<div class="clear"></div>
	<div class="section">
	<span class="Title"><b>Publications (<a href="https://scholar.google.com/citations?user=Tre69v0AAAAJ&hl=zh-CN" target="_blank" rel="nofollow">ALL </a>)</b></span><p><p>
	<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
	(* Equal contribution, † Corresponding author)  


		<!-- PointMamba -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/PointMamba.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					PointMamba: A Simple State Space Model for Point Cloud Analysis
				</div>
				<div class="paperName">
					<b>Dingkang Liang*</b>, Xin Zhou*, Wei Xu, Xingkui Zhu, Zhikang Zou, Xiaoqing Ye, Xiao Tan, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>Arxiv</b></span>, 2024.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2402.10739" target="_blank" rel="nofollow">Paper</a>	
					| <a href="https://github.com/LMD0311/PointMamba" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>
	

		<!-- DAPT -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/DAPT.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis
				</div>
				<div class="paperName">
					Xin Zhou*, <b>Dingkang Liang</b>*, Wei Xu, Xingkui Zhu, Yihan Xu, Zhikang Zou, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>CVPR</b></span>, 2024.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2403.01439" target="_blank" rel="nofollow">Paper</a>	
					| <a href="https://github.com/LMD0311/DAPT" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>


		<!-- LATFormer -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/LATFormer.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					LATFormer: Locality-Aware Point-View Fusion Transformer for 3D Shape Recognition
				</div>
				<div class="paperName">
					Xinwei He*, Silin Cheng*, <b>Dingkang Liang*</b>, Song Bai, Xi Wang, Yingying Zhu
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>Pattern Recognition</b></span>, 2024.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2109.01291" target="_blank" rel="nofollow">Paper</a>
					
				</div>
			</div>
		</div>			


		<!-- ViT-WSS3D -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/ViT-WSS3D.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					A Simple Vision Transformer for Weakly Semi-supervised 3D Object Detection
				</div>
				<div class="paperName">
					Dingyuan Zhang*, <b>Dingkang Liang*</b>, Zhikang Zou*, Jingyu Li, Xiaoqing Ye, Zhe Liu, Xiao Tan, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>ICCV</b></span>, 2023.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_A_Simple_Vision_Transformer_for_Weakly_Semi-supervised_3D_Object_Detection_ICCV_2023_paper.html" target="_blank" rel="nofollow">Paper</a>
				</div>
			</div>
		</div>
	

		<!--CrowdCLIP	 -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/CrowdCLIP.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					CrowdCLIP: Unsupervised Crowd Counting via Vision-Language Model
				</div>
				<div class="paperName">
					<b>Dingkang Liang*</b>, Jiahao Xie*, Zhikang Zou, Xiaoqing Ye, Wei Xu, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>CVPR</b></span>, 2023.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2304.04231" target="_blank" rel="nofollow">Paper</a>
					| <a href="https://github.com/dk-liang/CrowdCLIP" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>
	

		
		<!-- SOOD -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/SOOD.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					SOOD: Towards Semi-Supervised Oriented Object Detection
				</div>
				<div class="paperName">
					Wei Hua*, <b>Dingkang Liang*</b>, Jingyu Li, XiaoLong Liu, Zhikang Zou, Xiaoqing Ye, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>CVPR</b></span>, 2023.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2304.04515" target="_blank" rel="nofollow">Paper</a>
					| <a href="https://github.com/HamPerdredes/SOOD" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>
		

		<!-- SAM3D -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/SAM3D.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model
				</div>
				<div class="paperName">
					DIngyuan Zhang, <b>Dingkang Liang</b>, Hongcheng Yang, Zhikang Zou, Xiaoqing Ye, Zhe Liu, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>SCIS</b></span>, 2023.<br> 
				</div>
				<div class="paperLink">
			
					| <a href="https://arxiv.org/abs/2306.02245" target="_blank" rel="nofollow">Paper</a>
					| <a href="https://github.com/DYZhang09/SAM3D" target="_blank" rel="nofollow">Code</a>
					
				</div>
			</div>
		</div>		

				

		<!-- QTNet -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/QTNet.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					Query-based Temporal Fusion with Explicit Motion for 3D Object Detection
				</div>
				<div class="paperName">
					Jinghua Hou*, Zhe Liu*, <b>Dingkang Liang</b>, Zhikang Zou, Xiaoqing Ye, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>NeurIPS</b></span>, 2023.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://openreview.net/forum?id=gySmwdmVDF" target="_blank" rel="nofollow">Paper</a>
					| <a href="https://github.com/AlmoonYsl/QTNet" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>

		<!-- DDS3D -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/DDS3D.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					DDS3D: Dense Pseudo-Labels with Dynamic Threshold for Semi-Supervised 3D Object Detection
				</div>
				<div class="paperName">
					Jingyu Li*, Zhe Liu*, Jinghua  Hou, <b>Dingkang Liang†</b>
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>IJCV</b></span>, 2022.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/1912.09632" target="_blank" rel="nofollow">Paper</a>		
					| <a href="https://github.com/dk-liang/AutoScale" target="_blank" rel="nofollow">Code</a>
				</div>
			</div>
		</div>

		<!-- CLTR -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/CLTR.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					An End-to-End Transformer Model for Crowd Localization
				</div>
				<div class="paperName">
					<b>Dingkang Liang</b>, Wei Xu, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>ECCV</b></span>, 2022.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2202.13065" target="_blank" rel="nofollow">Paper</a>
					| <a href="https://github.com/dk-liang/CLTR" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>


		<!-- CAN -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/CAN.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					When Counting Meets HMER: Counting-Aware Network for Handwritten Mathematical Expression Recognition
				</div>
				<div class="paperName">
					Bohan Li*, Ye Yuan*, <b>Dingkang Liang</b>, Xiao Liu, Zhilong Ji, Jinfeng Bai, Wenyu Liu, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>ECCV</b></span>, 2022.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2207.11463" target="_blank" rel="nofollow">Paper</a>
					| <a href="https://github.com/LBH1024/CAN" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>


		<!-- FIDTM -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/FIDTM.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					Focal inverse distance transform maps for crowd localization
				</div>
				<div class="paperName">
					<b>Dingkang Liang</b>, Wei Xu, Yingying Zhu, Yu Zhou
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>ESI Highly Cited Paper</b></span><br> 
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>IEEE TMM</b></span>, 2022.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://ieeexplore.ieee.org/abstract/document/9875106/" target="_blank" rel="nofollow">Paper</a>							
					| <a href="https://github.com/dk-liang/FIDTM" target="_blank" rel="nofollow">Code</a>
				</div>
			</div>
		</div>


		<!-- TransCrowd -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/TransCrowd.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					TransCrowd: weakly-supervised crowd counting with transformers
				</div>
				<div class="paperName">
					<b>Dingkang Liang</b>, Xiwu Chen, Wei Xu, Yu Zhou, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>ESI Highly Cited Paper</b></span><br> 
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>SCIS</b></span>, 2022.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://link.springer.com/article/10.1007/s11432-021-3445-y" target="_blank" rel="nofollow">Paper</a>		
					| <a href="https://github.com/dk-liang/TransCrowd" target="_blank" rel="nofollow">Code</a>
				</div>
			</div>
		</div>


		<!-- AutoScale -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/AutoScale.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					AutoScale: Learning to Scale for Crowd Counting
				</div>
				<div class="paperName">
					Chenfeng Xu*, <b>Dingkang Liang*</b>, Yongchao Xu, Song Bai, Wei Zhan, Xiang Bai, Massayoshi Tomizuka
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>IJCV</b></span>, 2022.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/1912.09632" target="_blank" rel="nofollow">Paper</a>		
					| <a href="https://github.com/dk-liang/AutoScale" target="_blank" rel="nofollow">Code</a>
				</div>
			</div>
		</div>

			
		
<div class="section">
				<span class="Title"><b>Academic Services (Reviewer)</b></span><p>
				<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		
				<div class="paperName"><b>

					• IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021 - 2024   <br>
					• IEEE/CVF International Conference on Computer Vision (ICCV), 2021 - 2023   <br>
					• European Conference on Computer Vision (ECCV), 2022   <br>
					• Neural Information Processing Systems (NeurIPS), 2023-2024   <br>
					• International Conference on Learning Representations (ICLR), 2024     <br>
					• International Conference on Machine Learning (ICML), 2024    <br>
					• AAAI Conference on Artificial Intelligence (AAAI), 2024   <br>
					• ACM International Conference on Multimedia (ACM MM), 2023 - 2024   <br>
					• IEEE International Conference on Robotics and Automation (ICRA), 2024   <br>
					• International Conference on 3D Vision (3DV), 2022   <br>

					• IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)  <br>
					• International Journal of Computer Vision (IJCV)  <br>
					• IEEE Transactions on Image Processing (TIP)  <br>
					• IEEE Transactions on Intelligent Transportation Systems (TITS)  <br>
					• IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)  <br>
					• Science China-Information Science (SCIS) 
				</div>		
		


				<span class="Title"><b>Others</b></span><p>
				<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
			
				<div class="paperName"><b>
	
					•Excellent volunteer teacher, Hainan, 2016.  <br>
					•Former CEO & Co-Founder, [Wefly](https://www.tianyancha.com/company/3176757052), Inc. (The status of Wefly is cancellation.) <br>
					•Vice-chairman, Science and Technology Association of College of EOAST, Nanjing University of Posts and Telecommunication, May 2017 ~ May 2018. <br>
					•Outstanding Student Leader. <br>
					•Monitor, Electromagnetic fields and wireless technology, Nanjing University of Posts and Telecommunication, Sep 2015 ~ Jun 2019. (Our class won the award of the advanced class of Jiangsu Province.) <br>
				</div>		
			
			
		
				
			<!-- site visitors begjin -->
			<div style="margin:50px 0;">
				<a href="https://clustrmaps.com/site/1bh1a" title="Visit tracker"><img src="//clustrmaps.com/map_v2.js?d=f4MP0UkdUkFjU8rhQtVNPbOqJDbrXXpe90qfkwCEcgU&cl=ffffff&w=a" /></a>
			</div>
			<!-- site visitors end -->
		

			<!-- Last update time begjin -->
			<div style="border-top: 3px solid #555; text-align: center;">
				<p style="color: #555;">Last updated: 2024-05-26</p>
			</div>
			<!-- Last update time end -->

	</div>
	
</body>
</html>
