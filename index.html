<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
	<style>
		body{
			color: #333;
		}
		#container{
			min-width: 1000px;
			width: 1000px;
/*			overflow: auto;
*/			margin: 50px auto;padding: 30px;
			/*zoom: 1;*/
*//*			border: 1px solid #ccc;background: #fc9;color: #fff;
*/		}
		#left{
			float: left;
			width: 230px;
			height: 250px;
			margin-left: 0px;
		}
		#right{
			float: left;
			width: auto;
			margin-left: 30px;
		}
		#name{
			font-size: 22.0pt;
		    mso-bidi-font-size: 24.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		        font-weight: bold;
		}
		#info{
		    font-size: 16.0pt;
		    mso-bidi-font-size: 16.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    margin-top: 30px;
		    margin-left: 5px;
		    margin-bottom: 10px;
		    
		}
		.clear{clear:both; height: 0; line-height: 0; font-size: 0}
		.Bio{
			font-size:16.0pt;
			mso-bidi-font-size:17.0pt;
			line-height:150%;
			font-family:Times;
			mso-bidi-font-family:Lato-Regular;
			text-align: justify;
		}
		span.SpellE {
		    mso-style-name: "";
		    mso-spl-e: yes;
		}
		span.Title{
			    font-size: 22.0pt;
			    mso-bidi-font-size: 17.0pt;
			    font-family: Times;
			    mso-bidi-font-family: Lato-Regular;
			    font: bold;
			    margin-top: 10px;
			    width: 1000px;
		}
		div.section{
			padding-top: 30px;
		}
		
		div.sub-left{
			float: left;
			width: 250px;
						
		}
		div.sub-left img{
			vertical-align: middle;
			horizontal-align: middle;
			margin-top: 10px;
		}
		
		div.sub-left span{
			height: 100%;
			display: inline-block;
			vertical-align: top;
						
		}
		div.sub-right{
			float: left;
			width: 750px;			
		}
		.paper{
			overflow: auto;
			zoom:1;
			border-top: 2px ;
			padding-bottom: 0px;
			min-height: 160px;

		}
		.paperTitle{
			font-size:14pt;
			mso-bidi-font-size:14pt;
			font-family:Calibri;
			mso-bidi-font-family:Calibri;
			margin-top: 10px;
			margin-bottom: 5px;
			font-weight: bold;
		}
		.paperName{
		    font-size: 12pt;
		    mso-bidi-font-size: 12pt;		    
		    font-family: Calibri;
		    mso-bidi-font-family: Times;
		    line-height:160%;
		    font-style: italic;
		}		
		.paperPub{
		    font-size: 14pt;
		    mso-bidi-font-size: 14pt;		    
		    font-family: Calibri;
		    mso-bidi-font-family: Times;		    
		    font-style: italic;
		    line-height:160%;
		}
		.paperLink{
		    font-size: 13.0pt;
		    mso-bidi-font-size: 13.0pt;		    
		    font-family: Calibri;
		    mso-bidi-font-family: Times;
		    line-height:170%;
		}
		.special{
		    margin-top: 0in;
		    margin-bottom: 0in;
		    margin-left: -.9pt;
		    margin-bottom: .0001pt;
		    text-indent: .9pt;
		    mso-pagination: none;
		    tab-stops: 13.75in;
		    mso-layout-grid-align: none;
		    text-autospace: none;
		}
		.long div.sub-left, .long div.sub-right{
			height: 300px;
			width: 980px;

		}
		.short div.sub-left, .short div.sub-right{
			height:150px;

		}
		div.sub-left,div.sub-right{
			height:200px;

		}
	</style>
</head>
<body>
	<div id="container">
		<div id="left">			
			<img width="230" height="230" src="personal_imgs/Sanya.jpg">
		</div>
		<div id="right">
			<div id="name"><span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang</b></span> (梁定康) </div>
			<div id="info">
				Ph.D student<p>
				Huazhong University of Science and Technology<p>
				Email: dkliang@hust.edu.cn<p>				
			</div>
		
			     <a href="https://github.com/dk-liang" target="_blank" rel="nofollow"><span>Github</span></a>  |
				 <a href="https://scholar.google.com/citations?user=Tre69v0AAAAJ&hl=zh-CN" target="_blank" rel="nofollow"><span>Google Scholar</span></a> |
				 <!-- <a href="Dingkang_Liang_CV.pdf" target="_blank" rel="nofollow"><span>CV</span></a>   -->
			
			</div>

		<div class="clear"></div>
		<div class="section">
			<span class="Title"><b>Brief Bio</b></span><p>			
				<div class="Bio">

					I received my Ph.D. degree from Huazhong University of Science and Technology in 2025, supervised by Prof. <a href="https://scholar.google.com/citations?user=UeltiQ4AAAAJ&hl=zh-CN" target="_blank" rel="nofollow">Xiang Bai</a>.
					My research interests mainly lie in Embodied AI, 3D Vision, World Model and Dense Object Analysis. 
					<br><br> 
					I was selected for the Youth Student Fundamental Research Project from NSFC (首批国家自然科学基金博士生项目入选者) and the Young Elite Scientists Sponsorship Program-Doctoral Student Special Plan from CAST (首批中国科协青年人才托举工程-博士生专项入选者), with a total funding of <span style="color:red">  <b> 340,000 RMB (~ 47,000 USD)</b></span>. 
					Additionally, I have won multiple championships in top-tier computer vision competitions, which earned me a prize of <span style='color:red'> <b>200,000 RMB (~ 28,000 USD)</b></span>. <br> <br> 

					I am open to research collaborations and internship opportunities. I have also mentored several undergraduate and graduate students who successfully published their work in top-tier conferences and journals. Please feel free to reach out to me via Email (dkliang@hust.edu.cn) or WeChat (liangdingkang)
		<!-- <li><b>PhD:</b> Huazhong University of Science and Technology (2022–), under the supervision of Prof. <a href="https://scholar.google.com/citations?user=UeltiQ4AAAAJ&hl=en" target="_blank">Xiang Bai</a>.</li>
		<li><b>MA:</b> Huazhong University of Science and Technology (2019–2022), under the supervision of Prof. <a href="https://scholar.google.com/citations?user=UeltiQ4AAAAJ&hl=en" target="_blank">Xiang Bai</a>, Prof. <a href="https://scholar.google.fr/citations?user=ArIg7-0AAAAJ&hl=fr" target="_blank">Yongchao Xu</a> and Prof. Yu Zhou</a>.</li>
		<li><b>BA:</b> Nanjing University of Posts and Telecommunication (2015–2019), under the supervision of Dr. Xiang Wan, Prof. Jian Xiao, and Prof. Yi Tong. -->
		</div>

	<div class="section">
		<span class="Title"><b>News</b></span>
		<p>
		<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		<div class="paper long"><b>
			<div class="sub-right"  style="height:auto; max-height:none; overflow:visible;" ></div>
			<div class="paperName" style="max-height: 600px; overflow-y: auto; padding-right: 10px;"><b>
				11 / 2025: I successfully <span style="color:red">defended my Ph.D. thesis</span> <br>
				11 / 2025: One paper is accepted by <span style="color:red">AAAI 2026 (oral)</span><br>
				10 / 2025: I'm awarded <span style="color:red">National Scholarship</span><br>
				09 / 2025: One paper is accepted by <span style="color:red">IEEE TPAMI</span><br> 
				09 / 2025: <b>Two</b> papers are accepted by <span style="color:red">NeurIPS 2025</span><br>
				09 / 2025: I was selected for the <span style="color:red"> ICCV 2025 Doctoral Consortium </span><br>  
				07 / 2025: One paper is accepted by <span style="color:red">IEEE TPAMI</span><br> 
				07 / 2025: One paper is accepted by <span style="color:red">ACM MM 2025</span><br>  
				06 / 2025: Two papers are accepted by <span style="color:red">ICCV 2025</span><br>  
				05 / 2025: <span style="color:red">Outstanding reviewer (6%) at CVPR 2025</span><br> 
				04 / 2025: TransCrowd is selected as an <span style="color:red">Outstanding Paper (3.4%)</span> in Science China Information Science (<span style="color:red">CCF A</span>).<br>
				04 / 2025: We win <span style="color:red">2nd</span> place in SoccerNet Challenges on the Monocular Depth Estimation Track (<span style="color:red">CVPR 2025</span>).<br>
				04 / 2025: I give a talk about VLM-based Driving Models in VOYAH company. <br>
				03 / 2025: I give a talk about 3D vision in CSIG Sharing Forum. <br>
				02 / 2025: Three papers are accepted by <span style="color:red">CVPR 2025</span><br>  
				01 / 2025: One paper is accepted by <span style="color:red">ICLR 2025</span><br>
				01 / 2025: I was supported by the Young Elite Scientists Sponsorship Program-Doctoral Student Special Plan by CAST (<span style="color:red"> <b>首批中国科协青年人才托举工程-博士生专项</b></span>), which is a grant of <b>40,000 RMB (~ 5,500 USD)</b>.<br>
				09 / 2024: I'm awarded <span style="color:red">National Scholarship</span><br>
				09 / 2024: I give a talk about dense object analysis in CSIG Student Member Sharing Forum. <br>
				09 / 2024: <b>Three</b> papers are accepted by <span style="color:red">NeurIPS 2024</span><br>
				09 / 2024: We win <span style="color:red">1st</span> place in the <span style="color:red">ECCV 2024</span> FishNet Classification Challenge.<br>
				08 / 2024: We win <span style="color:red">2nd</span> place in The First Dataset Distillation Challenge (<span style="color:red">ECCV 2024</span>) on the Fixed IPC Track.<br>
				07 / 2024: One paper is accepted by <span style="color:red">ECCV 2024</span><br>
				04 / 2024: I was supported by the Youth Student Fundamental Research Project from NSFC (<span style="color:red"> <b>首批国家自然科学基金博士生项目</b></span>), which is a grant of <b>300,000 RMB (~ 41,700 USD)</b>.<br>
				03 / 2024: FIDTM is selected as an <span style="color:red">ESI Highly Cited Paper</span> (Top 1% of papers in the academic field)<br>
				<!-- 03 / 2024: One paper is accepted by <span style="color:red">PR</span><br> -->
				02 / 2024: One paper is accepted by <span style="color:red">CVPR 2024</span><br>
				09 / 2023: <span style="color:red">Outstanding reviewer (1.5%) at ICCV 2023</span><br>
				09 / 2023: One paper is accepted by <span style="color:red">NeurIPS 2023</span><br>
				09 / 2023: TransCrowd is selected as an <span style="color:red">ESI Highly Cited Paper</span> (Top 1% of papers in the academic field)<br>
				<!-- 09 / 2023: One paper is accepted by <span style="color:red">IEEE TII</span><br>
				08 / 2023: One paper is accepted by <span style="color:red">IEEE RAL</span><br> -->
				<!-- 08 / 2023: One paper is accepted by <span style="color:red">PRCV 2023</span><br> -->
				07 / 2023: One paper is accepted by <span style="color:red">ICCV 2023</span><br>
				<!-- 04 / 2023: One paper is accepted by <span style="color:red">ICDAR 2023</span><br> -->
				03 / 2023: <b>Two</b> papers are accepted by <span style="color:red">CVPR 2023</span><br>
				<!-- 02 / 2023: One paper is accepted by <span style="color:red">ICASSP 2023</span><br> -->
				01 / 2023: One paper is accepted by <span style="color:red">ICRA 2023</span><br>
				09 / 2022: Guided graduate students win <span style="color:red">1st</span> place in the VisDrone2022 (PRCV) challenge on the Crowd Counting Track.<br>
				08 / 2022: One paper is accepted by <span style="color:red">IEEE TMM</span><br>
				06 / 2022: Two papers are accepted by <span style="color:red">ECCV 2022</span><br>
				03 / 2022: We released the first comprehensive public African text dataset [<a href="https://dk-liang.github.io/HUST-ASTD/" target="_blank" rel="nofollow">project</a>]<br>
				10 / 2021: I'm awarded <span style="color:red">National Scholarship</span><br>
				10 / 2021: One paper is accepted by <span style="color:red">IJCV</span><br>
				09 / 2021: We win <span style="color:red">1st</span> place in The VisDrone2021 (<span style="color:red">ICCV</span>) Challenge on the Crowd Counting Track.<br>
				09 / 2020: We win <span style="color:red">1st</span> place in The VisDrone2020 (<span style="color:red">ECCV</span>) Challenge on the Crowd Counting Track.<br>
				11 / 2019: We win <span style="color:red">1st</span> place in The CV101 (held by Extremevision and Intel), obtaining 180,000 RMB Bonus.<br>
			</b></div>
			</b></div>
		</b></div>
	</div>
				

	<div class="clear"></div>
	<div class="section">
	<span class="Title"><b>Wechat (welcome any discussion)</span><p><p>
	<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
	<div class="sub-left">
		<span></span>
		<img border="0" width="180" height="200" src="imgs/wechat.jpg">
	</div>



	
	<div class="clear"></div>
	<div class="section">
	<span class="Title"><b>Selected publications (<a href="https://scholar.google.com/citations?user=Tre69v0AAAAJ&hl=zh-CN" target="_blank" rel="nofollow">ALL </a>)</b></span><p><p>
	<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
	(* Co-first author, # Corresponding author, + Project leader) <br> <br>

		<!-- Video4Edit -->
		<div class="paper short" style="display: flex; align-items: center;">
			<div class="sub-left" style="margin-right: 20px;">
				<img border="0" width="200" height="130" src="imgs/Video4Edit.png">
			</div>
			<div class="sub-right" style="flex-grow: 1;">
				<div class="paperTitle">
					Video4Edit: Viewing Image Editing as a Degenerate Temporal Process
				</div>
				<div class="paperName">
					Xiaofan Li, Yanpeng Sun, Chenming Wu, YuAn Wang, Fan Duan, Weihao Bo, Yumeng Zhang, <span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang</b></span>
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>arxiv</b></span> <br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2511.18131" target="_blank" rel="nofollow">Paper</a>	
					| <a href="" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>
		

		<!-- FVAR -->
		<div class="paper short" style="display: flex; align-items: center;">
			<div class="sub-left" style="margin-right: 20px;">
				<img border="0" width="200" height="130" src="imgs/FVAR.png">
			</div>
			<div class="sub-right" style="flex-grow: 1;">
				<div class="paperTitle">
					FVAR: Visual Autoregressive Modeling via Next Focus Prediction
				</div>
				<div class="paperName">
				Xiaofan Li, Chenming Wu, Yanpeng Sun, Jiaming Zhou, Delin Qu, Yansong Qu, Weihao Bo, Haibao Yu, <span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang</b></span>
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>arxiv</b></span><br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2511.18838" target="_blank" rel="nofollow">Paper</a>	
					| <a href="" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>
	
		<!-- GRANT -->
		<div class="paper short" style="display: flex; align-items: center;">
			<div class="sub-left" style="margin-right: 20px;">
				<img border="0" width="200" height="130" src="imgs/Grant.png">
			</div>
			<div class="sub-right" style="flex-grow: 1;">
				<div class="paperTitle">
					Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution
				</div>
				<div class="paperName">
					<span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang</b></span>, Cheng Zhang, Xiaopeng Xu, Jianzhong Ju, Zhenbo Luo, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>AAAI (Oral)</b></span>, 2026.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2511.19430" target="_blank" rel="nofollow">Paper</a>	
					| <a href="https://github.com/H-EmbodVis/GRANT" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>

		<!-- UniFuture -->
		<div class="paper short" style="display: flex; align-items: center;">
			<div class="sub-left" style="margin-right: 20px;">
				<img border="0" width="200" height="130" src="imgs/UniFuture.jpg">
			</div>
			<div class="sub-right" style="flex-grow: 1;">
				<div class="paperTitle">
					Seeing the Future, Perceiving the Future: A Unified Driving World Model for Future Generation and Perception
				</div>
				<div class="paperName">
					<span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang</b></span>, Dingyuan Zhang, Xin Zhou, Sifan Tu, Tianrui Feng, Xiaofan Li, Yumeng Zhang, Mingyang Du, Xiao Tan, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>Arxiv</b></span>, 2025. <a href="https://arxiv.org/abs/2503.13587" target="_blank" rel="nofollow">Paper</a>. <a href="https://github.com/dk-liang/UniFuture" target="_blank" rel="nofollow">Code</a>. <br> 
				</div>
			</div>
		</div>

		<!-- PointMamba -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/PointMamba.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					PointMamba: A Simple State Space Model for Point Cloud Analysis
				</div>
				<div class="paperName">
					<span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang</b></span>, Xin Zhou, Wei Xu, Xingkui Zhu, Zhikang Zou, Xiaoqing Ye, Xiao Tan, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>NeurIPS</b></span>, 2024.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2402.10739" target="_blank" rel="nofollow">Paper</a>	
					| <a href="https://github.com/LMD0311/PointMamba" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>

		<!-- SOOD++ -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/SOOD++.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					SOOD++: Leveraging Unlabeled Data to Boost Oriented Object Detection
				</div>
				<div class="paperName">
					<span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang</b></span>, Wei Hua, Chunsheng Shi, Zhikang Zou, Xiaoqing Ye, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>IEEE TPAMI</b></span>, 2025.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2407.01016" target="_blank" rel="nofollow">Paper</a>	
					| <a href="https://github.com/HamPerdredes/SOOD" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>

		<!-- PointGST -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/PointGST.jpg">
			</div>
			<div class="sub-right" style="flex-grow: 1;">
				<div class="paperTitle">
					Parameter-Efficient Fine-Tuning in Spectral Domain for Point Cloud Learning
				</div>
				<div class="paperName">
					<span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang</b></span>, Tianrui Feng, Xin Zhou, Yumeng Zhang, Zhikang Zou, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>IEEE TPAMI</b></span>, 2025. <br> 
								</div>
				<div class="paperLink">
					｜<a href="https://arxiv.org/abs/2410.08114" target="_blank" rel="nofollow">Paper</a>
					｜<a href="https://github.com/jerryfeng2003/PointGST" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>


		<!-- HERMES -->
		<div class="paper short" style="display: flex; align-items: center;">
			<div class="sub-left" style="margin-right: 20px;">
				<img border="0" width="200" height="130" src="imgs/HERMES.jpg">
			</div>
			<div class="sub-right" style="flex-grow: 1;">
				<div class="paperTitle">
					HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation
				</div>
				<div class="paperName">
					Xin Zhou*, <span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang*</b></span>, Sifan Tu, Xiwu Chen, Yikang Ding, Dingyuan Zhang, Feiyang Tan, Hengshuang Zhao, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>ICCV</b></span>, 2025. <a href="https://arxiv.org/abs/2501.14729" target="_blank" rel="nofollow">Paper</a>. <a href="https://github.com/LMD0311/HERMES" target="_blank" rel="nofollow">Code</a>. <br> 
				</div>
			</div>
		</div>

		<!--CrowdCLIP	 -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/CrowdCLIP.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					CrowdCLIP: Unsupervised Crowd Counting via Vision-Language Model
				</div>
				<div class="paperName">
					<span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang</b></span>, Jiahao Xie, Zhikang Zou, Xiaoqing Ye, Wei Xu, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>CVPR</b></span>, 2023.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2304.04231" target="_blank" rel="nofollow">Paper</a>
					| <a href="https://github.com/dk-liang/CrowdCLIP" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>

		<!-- CLTR -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/CLTR.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					An End-to-End Transformer Model for Crowd Localization
				</div>
				<div class="paperName">
					<span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang</b></span>, Wei Xu, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>ECCV</b></span>, 2022.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2202.13065" target="_blank" rel="nofollow">Paper</a>
					| <a href="https://github.com/dk-liang/CLTR" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>

		<!-- ToC3D -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/ToC3D.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					Make Your ViT-based Multi-view 3D Detectors Faster via Token Compression
				</div>
				<div class="paperName">
					Dingyuan Zhang*, <span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang*</b></span>, Zichang Tan, Xiaoqing Ye, Cheng Zhang, Jingdong Wang, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>ECCV</b></span>, 2024.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2409.00633" target="_blank" rel="nofollow">Paper</a>    
					| <a href="https://github.com/DYZhang09/ToC3D" target="_blank" rel="nofollow">Code</a>    
				</div>
			</div>
		</div>

		<!-- FIDTM -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/FIDTM.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					Focal inverse distance transform maps for crowd localization
				</div>
				<div class="paperName">
					<span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang</b></span>, Wei Xu, Yingying Zhu, Yu Zhou
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>ESI Highly Cited Paper</b></span><br> 
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>IEEE TMM</b></span>, 2022.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://ieeexplore.ieee.org/abstract/document/9875106/" target="_blank" rel="nofollow">Paper</a>							
					| <a href="https://github.com/dk-liang/FIDTM" target="_blank" rel="nofollow">Code</a>
				</div>
			</div>
		</div>

		<!-- TransCrowd -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/TransCrowd.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					TransCrowd: weakly-supervised crowd counting with transformers
				</div>
				<div class="paperName">
					<span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang</b></span>, Xiwu Chen, Wei Xu, Yu Zhou, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>ESI Highly Cited Paper</b></span><br> 
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>SCIS</b></span>, 2022.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://link.springer.com/article/10.1007/s11432-021-3445-y" target="_blank" rel="nofollow">Paper</a>		
					| <a href="https://github.com/dk-liang/TransCrowd" target="_blank" rel="nofollow">Code</a>
				</div>
			</div>
		</div>


		<!-- DAPT -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/DAPT.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis
				</div>
				<div class="paperName">
					Xin Zhou*, <span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang*</b></span>, Wei Xu, Xingkui Zhu, Yihan Xu, Zhikang Zou, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>CVPR</b></span>, 2024.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2403.01439" target="_blank" rel="nofollow">Paper</a>	
					| <a href="https://github.com/LMD0311/DAPT" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>	

		<!-- ViT-WSS3D -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/ViT-WSS3D.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					A Simple Vision Transformer for Weakly Semi-supervised 3D Object Detection
				</div>
				<div class="paperName">
					Dingyuan Zhang*, <span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang*</b></span>, Zhikang Zou*, Jingyu Li, Xiaoqing Ye, Zhe Liu, Xiao Tan, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>ICCV</b></span>, 2023.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_A_Simple_Vision_Transformer_for_Weakly_Semi-supervised_3D_Object_Detection_ICCV_2023_paper.html" target="_blank" rel="nofollow">Paper</a>
					| <a href="https://github.com/DYZhang09/ViTWSS3D" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>
		
		<!-- SOOD -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/SOOD.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					SOOD: Towards Semi-Supervised Oriented Object Detection
				</div>
				<div class="paperName">
					Wei Hua*, <span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang*</b></span>, Jingyu Li, XiaoLong Liu, Zhikang Zou, Xiaoqing Ye, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>CVPR</b></span>, 2023.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2304.04515" target="_blank" rel="nofollow">Paper</a>
					| <a href="https://github.com/HamPerdredes/SOOD" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>

		<!-- LATFormer -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/LATFormer.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					LATFormer: Locality-Aware Point-View Fusion Transformer for 3D Shape Recognition
				</div>
				<div class="paperName">
					Xinwei He*, Silin Cheng*, <span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang*</b></span>, Song Bai, Xi Wang, Yingying Zhu
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>Pattern Recognition</b></span>, 2024.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2109.01291" target="_blank" rel="nofollow">Paper</a>
					
				</div>
			</div>
		</div>		

		<!-- AutoScale -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/AutoScale.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					AutoScale: Learning to Scale for Crowd Counting
				</div>
				<div class="paperName">
					Chenfeng Xu*, <span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang*</b></span>, Yongchao Xu, Song Bai, Wei Zhan, Xiang Bai, Massayoshi Tomizuka
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>IJCV</b></span>, 2022.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/1912.09632" target="_blank" rel="nofollow">Paper</a>		
					| <a href="https://github.com/dk-liang/AutoScale" target="_blank" rel="nofollow">Code</a>
				</div>
			</div>
		</div>


		<!-- ORION -->
		<div class="paper short" style="display: flex; align-items: center;">
			<div class="sub-left" style="margin-right: 20px;">
				<img border="0" width="200" height="130" src="imgs/ORION.jpg">
			</div>
			<div class="sub-right" style="flex-grow: 1;">
				<div class="paperTitle">
					ORION: A Holistic End-to-End Autonomous Driving Framework by Vision-Language Instructed Action Generation
				</div>
				<div class="paperName">
					Haoyu Fu, Diankun Zhang, Zongchuang Zhao, Jianfeng Cui, <span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang+</b></span>, Chong Zhang, Dingyuan Zhang, Hongwei Xie+, Bing Wang, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>ICCV</b></span>, 2025. <a href="https://arxiv.org/abs/2503.19755" target="_blank" rel="nofollow">Paper</a>. <a href="https://github.com/xiaomi-mlab/Orion" target="_blank" rel="nofollow">Code</a>. <br> 
				</div>
			</div>
		</div>

		<!-- DiVerse -->
		<div class="paper short" style="display: flex; align-items: center;">
			<div class="sub-left" style="margin-right: 20px;">
				<img border="0" width="200" height="130" src="imgs/DiVerse.png">
			</div>
			<div class="sub-right" style="flex-grow: 1;">
				<div class="paperTitle">
					Navigation World Model for Driving Simulation via Multimodal Trajectory Prompting and Motion Alignment
				</div>
				<div class="paperName">
				Xiaofan Li, Chenming Wu, Zhao Yang, Zhihao Xu, <span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang</b></span>, Yumeng Zhang, Ji Wan, Jun Wang
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>ACM MM</b></span>, 2025. <a href="https://arxiv.org/abs/2504.18576" target="_blank" rel="nofollow">Paper</a>. <a href="https://github.com/shalfun/DriVerse" target="_blank" rel="nofollow">Code</a>. <br> 
				</div>
			</div>
		</div>

		<!-- DriveMonkey -->
		<div class="paper short" style="display: flex; align-items: center;">
			<div class="sub-left" style="margin-right: 20px;">
				<img border="0" width="200" height="130" src="imgs/drivemonkey.jpg">
			</div>
			<div class="sub-right" style="flex-grow: 1;">
				<div class="paperTitle">
					Extending Large Vision-Language Model for Diverse Interactive Tasks in Autonomous Driving
				</div>
				<div class="paperName">
					Zongchuang Zhao, Haoyu Fu, <span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang+</b></span>, Xin Zhou, Dingyuan Zhang, Hongwei Xie, Bing Wang, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>Arxiv</b></span>, 2025. <a href="https://arxiv.org/abs/2505.08725" target="_blank" rel="nofollow">Paper</a>. <a href="https://github.com/zc-zhao/DriveMonkey" target="_blank" rel="nofollow">Code</a>. <br> 
				</div>
			</div>
		</div>

		<!-- Survey -->
		<div class="paper short" style="display: flex; align-items: center;">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/survey.jpg">
			</div>
			<div class="sub-right" style="flex-grow: 1;">
				<div class="paperTitle">
					The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey
				</div>
				<div class="paperName">
					Sifan Tu, Xin Zhou, <span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang</b></span>, Xingyu Jiang, Yumeng Zhang, Xiaofan Li, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>Arxiv</b></span>, 2025.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2502.10498" target="_blank" rel="nofollow">Paper</a>  
					| <a href="https://github.com/LMD0311/Awesome-World-Model" target="_blank" rel="nofollow">Code</a>  
				</div>
			</div>
		</div>

		<!-- MINIMA -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/MINIMA.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					MINIMA: Modality Invariant Image Matching
				</div>
				<div class="paperName">
					Xingyu Jiang, Jiangwei Ren, Zizhuo Li, Xin Zhou, <span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang</b></span>, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>CVPR</b></span>, 2025.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2412.19412" target="_blank" rel="nofollow">Paper</a>
					| <a href="https://github.com/LSXI7/MINIMA" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>

		<!-- TIDE -->
		<!-- <div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/TIDE.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					A Unified Image-Dense Annotation Generation Model for Underwater Scenes
				</div>
				<div class="paperName">
					Hongkai Lin, <span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang</b></span>, Zhenghao Qi, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>CVPR</b></span>, 2025.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2503.21771" target="_blank" rel="nofollow">Paper</a>
					| <a href="https://github.com/HongkLin/TIDE?tab=readme-ov-file" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div> -->

		<!-- UniSeg3D -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/UniSeg3D.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					A Unified Framework for 3D Scene Understanding
				</div>
				<div class="paperName">
					Wei Xu, Chunsheng Shi, Sifan Tu, Xin Zhou, <span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang+</b></span> , Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>NeurIPS</b></span>, 2024.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2407.03263" target="_blank" rel="nofollow">Paper</a>	
					| <a href="https://github.com/dk-liang/UniSeg3D" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>

		<!-- QTNet -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/QTNet.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					Query-based Temporal Fusion with Explicit Motion for 3D Object Detection
				</div>
				<div class="paperName">
					Jinghua Hou, Zhe Liu, <span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang</b></span>, Zhikang Zou, Xiaoqing Ye, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>NeurIPS</b></span>, 2023.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://openreview.net/forum?id=gySmwdmVDF" target="_blank" rel="nofollow">Paper</a>
					| <a href="https://github.com/AlmoonYsl/QTNet" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>

		<!-- SAM3D -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/SAM3D.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model
				</div>
				<div class="paperName">
					DIngyuan Zhang, <span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang</b></span>, Hongcheng Yang, Zhikang Zou, Xiaoqing Ye, Zhe Liu, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>SCIS</b></span>, 2024.<br> 
				</div>
				<div class="paperLink">
			
					| <a href="https://arxiv.org/abs/2306.02245" target="_blank" rel="nofollow">Paper</a>
					| <a href="https://github.com/DYZhang09/SAM3D" target="_blank" rel="nofollow">Code</a>
					
				</div>
			</div>
		</div>	

		<!-- CAN -->
		<!-- <div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/CAN.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					When Counting Meets HMER: Counting-Aware Network for Handwritten Mathematical Expression Recognition
				</div>
				<div class="paperName">
					Bohan Li, Ye Yuan, <span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang</b></span>, Xiao Liu, Zhilong Ji, Jinfeng Bai, Wenyu Liu, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>ECCV</b></span>, 2022.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2207.11463" target="_blank" rel="nofollow">Paper</a>
					| <a href="https://github.com/LBH1024/CAN" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div> -->

		<!-- Mini-Monkey -->
		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/mini_monkey.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					Mini-Monkey: Multi-Scale Adaptive Cropping for Multimodal Large Language Models
				</div>
				<div class="paperName">
					Mingxin Huang, Yuliang Liu, <span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang</b></span>, Lianwen Jin, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>ICLR</b></span>, 2025.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2408.02034" target="_blank" rel="nofollow">Paper</a>	
					| <a href="https://github.com/Yuliang-Liu/Monkey" target="_blank" rel="nofollow">Code</a>	
				</div>
			</div>
		</div>

		<!-- MoE Jetpack -->
		<!-- <div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/MoE Jetpack.jpg">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					From Dense Checkpoints to Adaptive Mixture of Experts for Vision Tasks
				</div>
				<div class="paperName">
					Xingkui Zhu, Yiran Guan, <span style="font-size: 1.1em; font-weight: bold; color: blue;"><b>Dingkang Liang</b></span>, Yuchao Chen, Yuliang Liu, Xiang Bai
				</div>
				<div class="paperPub">
					<span style="color:red"> <b>NeurIPS</b></span>, 2024.<br> 
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/abs/2406.04801" target="_blank" rel="nofollow">Paper</a>	
					| <a href="https://github.com/Adlith/MoE-Jetpack" target="_blank" rel="nofollow">Code</a>	
			</div>
		</div>
		</div> -->


			
		
<div class="section">

				<span class="Title"><b>Competition</b></span><p>
					<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
					<div class="paperName"><b>
						<li> The <b>1st place</b> in the ECCV 2024 FishNet Classification Challenge<br>
						<li> The <b>2nd place</b> in The First Dataset Distillation Challenge (ECCV 2024) on the Fixed IPC Track<br>
						<li>The <b>4th place</b> of the 3D object detection track in CSIG-challenge 2022</li>
						<li><b>Silver Award</b>, China International College Students' "Internet+" Innovation and Entrepreneurship Competition (中国国际互联网+”大学生创新创业大赛全国总决赛), 2021.</li>
						<li>The <b>1st place</b> of the Crowd Counting track in Vision Meets Drone (VisDrone) challenge with ICCV 2021.</li>
						<li>The <b>1st place</b> of the Crowd Counting track in Vision Meets Drone (VisDrone) challenge with ECCV 2020.</li>
						<li>The <b>1st place</b> <a href="https://www.cvmart.net/list/ECV2019">(100,000 RMB Bonus)</a> of the Crowd Counting track in CV101 (held by Extremevision and Intel), Shenzhen, China, 2019.</li>
						<li>The <b>1st place</b> <a href="https://www.cvmart.net/list/ECV2019">(80,000 RMB Bonus)</a> of the OpenVino track in CV101 (held by Extremevision and Intel), Shenzhen, China, 2019.</li>
						<li><b>Gold Award</b>, China College Students' "Internet+" Innovation and Entrepreneurship Competition (中国互联网+”大学生创新创业大赛全国总决赛), 2019.</li>
						<li><b>Grand Prize</b>, "Challenge Cup" Competition, Provincial (挑战杯”江苏省大学生课外学术作品竞赛), 2019.</li>
						<li>全国大学生FPGA创新设计邀请赛国家级一等奖 <b>(3,000 RMB Bonus)</b>, 2019.</li>
						<li>国家级大学生创新训练计划 <b>(10,000 RMB Bonus)</b>，结题成绩优秀，入选第<b><a href="http://www.moe.gov.cn/s78/A08/tongzhi/201806/t20180604_338207.html">十一届全国大学生创新创业年会</a></b>参展项目, 2018.</li>
						<li>全国大学生物联网设计竞赛华东赛区一等奖, 2018.</li>
						<li>江苏人工智能创新创业大赛优秀奖<b>(10,000 RMB Bonus)</b>, 2018.</li>
						<li>全国大学生电子设计竞赛国家级二等奖, 2018.</li>
						<li>全国大学生FPGA创新设计邀请赛国家级二等奖, 2017.</li>
						<li>“英飞凌”杯全国高校无人机创新设计应用大赛 Top 3.5% (14/400) <b>(3,500 RMB Bonus)</b>, 2017.</li>
					</div>		


				<span class="Title"><b>Academic Services (Reviewer)</b></span><p>
				<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		
				<div class="paperName"><b>
					
				<span style="color:red">Outstanding reviewer at <a href="https://iccv2023.thecvf.com/outstanding.reviewers-118.php">ICCV 2023</a> and <a href="https://cvpr.thecvf.com/Conferences/2025/ProgramCommittee#all-outstanding-reviewer">CVPR 2025</a> </span> <br>
					

					• IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)   <br>
					• IEEE/CVF International Conference on Computer Vision (ICCV)  <br>
					• European Conference on Computer Vision (ECCV)  <br>
					• Neural Information Processing Systems (NeurIPS)  <br>
					• International Conference on Learning Representations (ICLR)   <br>
					• International Conference on Machine Learning (ICML)  <br>
					• AAAI Conference on Artificial Intelligence (AAAI) <br>
					• ACM International Conference on Multimedia (ACM MM) <br>
					• IEEE International Conference on Robotics and Automation (ICRA) <br>
					• International Conference on 3D Vision (3DV)  <br>
					<br>
					• IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)  <br>
					• International Journal of Computer Vision (IJCV)  <br>
					• IEEE Transactions on Image Processing (TIP)  <br>
					• IEEE Transactions on Intelligent Transportation Systems (TITS)  <br>
					• IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)  <br>
					• Science China-Information Science (SCIS) 
				</div>		
		


				<span class="Title"><b>Co-supervised Students</b></span><p>
					<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
					
					<p>As a passionate collaborator, I am always open to working with fellow researchers. I have had the privilege of co-supervising several talented students. If you are interested in collaborating, feel free to reach out to me.</p>
				
					<div class="paperName"><b>
						已毕业的研究生：<br>
						• Dingyuan Zhang. Master degree, graduating in 2025. Now at Xiaomi. 一作ICCV 23, ECCV 24, RAL 24, SCIS 23. <br>
						• Wei Hua. Master degree, graduating in 2024. Now at Jiangxi Electric Power Grid Corporation. 一作CVPR 23, ICDAR 23. <br>
						• Jingyu Li. Master degree, graduating in 2024. Now Ph.D. at Fudan University. 一作ICRA 23. <br>
						• Jianfeng Kuang. Master degree, graduating in 2023. Now at ByteDance. 一作ICDAR 23. <br>
					</div>

					<!-- <div class="paperName"><b>
						已毕业的本科生：<br>
						• Xin Zhou. Graduating in 2025. Now Ph.D at HUST. 一作CVPR 24, NeurIPS 24. <br>
						• Yihan Xu. Graduating in 2025. Now Ph.D at Tsinghua University. 参与CVPR 24. <br>
						• Xiaopeng Xu. Graduating in 2025. Now Ph.D at HUST. 参与CVPR 24. <br>
						• Xinhan Wang. Graduating in 2025. Now Ph.D at HUST. 参与CVPR 24. <br>
						• Tingting Yao. Graduating in 2024. Now Ph.D at George Mason University. <br>
						• Yuchao Chen. Graduating in 2024. Now Master student at HUST. <br>
						
					</div> -->



				</p>



				<!-- <span class="Title"><b>Others</b></span><p>
				<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
			
				<div class="paperName"><b>
					• Excellent volunteer teacher, Hainan, 2016.  <br>
					• Former CEO & Co-Founder, <a href="https://www.tianyancha.com/company/3176757052">Wefly</a>, Inc. (The company has since been dissolved.) <br>
					• Vice-chairman, Science and Technology Association of College of EOAST, Nanjing University of Posts and Telecommunication, May 2017 ~ May 2018. <br>
					• Outstanding Student Leader. <br>
					• Monitor, Electromagnetic fields and wireless technology, Nanjing University of Posts and Telecommunication, Sep 2015 ~ Jun 2019. (Our class won the award of the advanced class of Jiangsu Province.) <br> 
				</div>
			 -->

				
				<!-- <span class="Title"><b>Moments</b></span><p>
				<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
				<table>
					<tr>
					<td><img src="./personal_imgs/ICCV.jpg" width="320" height="260"  border=0 /></td>
					<td><img src="./personal_imgs/Bengio.jpg" width="320" height="260"  border=0 /></td>
					<td><img src="./personal_imgs/CV101.jpg" width="350" height="260"  border=0 /></td>
					</tr>
					<tr>
						<td><img src="./personal_imgs/2019.jpg" width="320" height="260"  border=0 /></td>
						<td><img src="./personal_imgs/2019-2.jpg" width="320" height="260"  border=0 /></td>
						<td><img src="./personal_imgs/yichang.jpg" width="350" height="260"  border=0 /></td>
					</tr>
				</table>  -->
	
				
				
				<!-- site visitors begin -->
				<div style="margin:50px 0;">
					<a href="https://clustrmaps.com/site/1bh1a" title="Visit tracker">
						<img src="https://clustrmaps.com/map_v2.png?d=f4MP0UkdUkFjU8rhQtVNPbOqJDbrXXpe90qfkwCEcgU&cl=ffffff&w=a" alt="Site Visitors Map" />
					</a>
				</div>
				<!-- site visitors end -->

		

			<!-- Last update time begjin -->
			<div style="border-top: 3px solid #555; text-align: center;">
				<p style="color: #555;">Last updated: 2025-11-16</p>
			</div>
			<!-- Last update time end -->

	</div>
	
</body>
</html>
